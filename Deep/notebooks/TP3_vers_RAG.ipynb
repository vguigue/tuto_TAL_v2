{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vers le Retrieval Augmented Generation\n",
    "---\n",
    "\n",
    "### Librement adapté des supports de Rémy DECOUPES\n",
    "\n",
    "---\n",
    "\n",
    "**Objectif** : \n",
    "1. Mise en place d'un système RAG (Retrieval Augmented Generation)\n",
    "2. Réaliser un topic modelling\n",
    "\n",
    "On repart des bases du TP2: vous devez savoir faire tourner ollama sur votre machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\") # 'gemma3:270m' pour aller plus vite\n",
    "\n",
    "# Envoi d’un prompt\n",
    "response = llm.invoke(\"Peux-tu lister les maladies des plantes à La Réunion ?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mise en place du RAG : Retrieval Augmented Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: les imports changent tout le temps... => StackOverFlow\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Mise en place du corpus\n",
    "\n",
    "Utilisation du jeu de données : https://huggingface.co/datasets/KisanVaani/agriculture-qa-english-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"KisanVaani/agriculture-qa-english-only\", split=\"train\")\n",
    "\n",
    "df = pd.DataFrame(ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"answers\"].to_list()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculer les embeddings des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "documents = [Document(page_content=text) for text in corpus[:200]]\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"which diseases can be spread by flies?\"\n",
    "searchDocs = vectorstore.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Préparer son LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) \n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",        # méthode pour combiner les docs\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/55/Question_Mark.svg\">\n",
    "\n",
    "**Questions** :\n",
    "\n",
    "---\n",
    "- Quel est l'argument k = 3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which diseases can be spread by flies?\"\n",
    "result = rag_chain({\"query\": query})\n",
    "\n",
    "print(\"Réponse générée :\\n\", result[\"result\"])\n",
    "print(\"\\nDocuments utilisés :\\n\", result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adapter votre système RAG\n",
    "\n",
    "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/55/Question_Mark.svg\">\n",
    "\n",
    "**Faîtes varier** :\n",
    "\n",
    "--- \n",
    "\n",
    "- Le nombre de documents utilisés par le contexte\n",
    "- Changer de jeu de données\n",
    "- Changer de modèle LLM\n",
    "- Changer de modèle d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vos tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Modelling\n",
    "\n",
    "L'objectif est d'identifier des sujets dans ce corpus de titre.\n",
    "Nous allons utiliser `BERTopic`.\n",
    "\n",
    "![img](https://maartengr.github.io/BERTopic/algorithm/default.svg)\n",
    "\n",
    "\n",
    "Ce paquet utilise donc tout ce qu'on a appris avec une configuration particulière de TF-IDF :\n",
    "\n",
    "![img2](https://maartengr.github.io/BERTopic/algorithm/c-TF-IDF.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons qu'il y a beaucoup de `stopwords` !\n",
    "\n",
    "Il est possible de tout configurer avec Bertopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(corpus[:200])\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
